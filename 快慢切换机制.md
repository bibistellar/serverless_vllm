# model_pool_serve 统一弹性模型池机制（Qwen3-VL-2B）

## 1. 目标与范围

目标：
- 构建 serverless 化弹性模型池，统一管理 `llama.cpp`（快）与 `vLLM`（慢）。
- 低负载/突发首波由 `llama.cpp` 快速承接，持续高负载由 `vLLM` 提供高吞吐。
- 在负载回落时自动释放冗余资源；在负载上升时自动扩容。
- 对客户端保持 OpenAI 兼容入口不变：`/v1/chat/completions`。

范围：
- 当前只考虑 `Qwen3-VL` 系列，优先 `Qwen3-VL-2B`。
- 先做单 alias 的双后端池化；多模型多租户是后续扩展。

---

## 2. 审查结论（现文档割裂点）

当前文档的问题不是方向错，而是“分层边界不清”：
- 已定义模型级快慢切换状态（`FAST_ONLY/WARMING/MIXED/VLLM_PRIMARY`）。
- 但 Worker 生命周期描述主要围绕 `vLLM`，未把 `llama.cpp` 纳入同一实例语义。
- 结果是“调度层有双后端，执行层像单后端”，实现上容易出现两套逻辑并行演进。

统一改造原则：
- 只保留一个决策主控：Serve 控制面（hybrid controller）。
- Worker 提供统一实例接口；后端差异通过“能力矩阵”约束，而不是两套状态机。
- 删除“完全释放但仍保留注册”的伪状态，完全释放即实例不存在。

---

## 3. 统一架构

### 3.1 分层职责

1. Serve 控制面（唯一主控）
- 维护模型级路由状态机。
- 根据指标决定扩容/缩容、切换、回退。
- 维护实例清单与放置策略（含软反亲和）。

2. Worker 执行面（统一实例接口）
- 执行 `start/wake/sleep/drain/delete/status`。
- 汇报实例状态、容量、TTFT/E2E、inflight。
- 不维护“模型副本计数”，只维护本机实例事实列表。

3. Autoscaler（从属控制环）
- 负责指标聚合与建议值计算。
- 不直接下发最终动作，由 Serve 主控裁决。

### 3.2 后端能力矩阵（统一语义的关键）

- `llama.cpp`：`start/running/drain/delete`，不支持分级 sleep。
- `vLLM`：`start/running/sleep_1/sleep_2/wake/drain/delete`。

统一接口不变；不支持的动作返回 `unsupported`，由主控走替代动作：
- 例如对 `llama.cpp` 的 `sleep` 请求，主控直接改用 `drain + delete`。

---

## 4. 统一状态机

### 4.1 模型级路由状态机（按 alias）

- `COLD`：无可用实例。
- `FAST_ONLY`：仅 `llama.cpp` 对外服务。
- `WARMING_VLLM`：`llama.cpp` 服务，后台拉起或唤醒 `vLLM`。
- `MIXED`：双后端分流，平滑切换窗口。
- `VLLM_PRIMARY`：`vLLM` 主承载，`llama.cpp` 仅兜底/缓冲。
- `DEGRADED_FAST`：`vLLM` 异常回退，仅 fast 服务并等待重试窗口。

核心转移：
1. `COLD -> FAST_ONLY`
- 首个请求到达且存在可用资源，先启动至少 1 个 `llama.cpp`。

2. `FAST_ONLY -> WARMING_VLLM`
- 连续 `N_up` 个窗口满足任一：
  - `inflight_fast >= C_up`
  - `queue_wait_p95 > queue_sla`
  - `e2e_p95_fast > e2e_sla`

3. `WARMING_VLLM -> MIXED`
- `vLLM` 连续 `N_ready` 次健康探测通过（`RUNNING/ACTIVE`）。

4. `MIXED -> VLLM_PRIMARY`
- 渐进分流完成（示例 `20% -> 50% -> 80% -> 100%`）且 `vLLM` 稳定。

5. `VLLM_PRIMARY -> FAST_ONLY`
- 低负载持续 `T_down`，并且 vLLM 在途为 0，允许慢后端降级（sleep 或删除）。

6. 任意状态 `-> DEGRADED_FAST`
- `vLLM` 不健康、启动失败、连续超时。

7. `DEGRADED_FAST -> WARMING_VLLM`
- 到达重试窗口后重新预热慢后端。

### 4.2 实例级生命周期（统一定义，按能力裁剪）

统一实例状态：
- `ABSENT`：实例不存在。
- `STARTING`：启动中。
- `RUNNING`：可接流量。
- `SLEEP_1`：轻睡眠（仅 vLLM）。
- `SLEEP_2`：深睡眠（仅 vLLM）。
- `DRAINING`：停止接新请求，等待在途清零。
- `DELETING`：资源异步释放中。
- `ERROR`：异常态。

允许转移（通用）：
- `ABSENT -> STARTING -> RUNNING`
- `RUNNING -> DRAINING -> DELETING -> ABSENT`
- `RUNNING -> ERROR`
- `ERROR -> STARTING`

vLLM 专属：
- `RUNNING <-> SLEEP_1 <-> SLEEP_2`
- `SLEEP_1/SLEEP_2 -> DRAINING/DELETING`

llama.cpp 专属：
- 无 `SLEEP_1/SLEEP_2`，空闲回收直接走 `DRAINING -> DELETING -> ABSENT`。

语义约束：
- 删除完成后必须从 Worker 实例表移除。
- 不再使用 `stopped` 挂名状态。

---

## 5. 统一调度策略（快承接 + 慢接管 + 弹性扩缩）

### 5.1 容量估算

优先运行时：
- `C_vllm = capacity`（来自 worker 上报）。

兜底估算：
- `C_vllm = floor(T_kv / T_req)`。

有效容量：
- `C_eff = floor(alpha * C_vllm)`，`alpha` 建议 `0.6~0.8`。
- `C_up = max(1, C_eff)`。
- `C_down = max(1, floor(beta * C_vllm))`，`beta` 建议 `0.25~0.4`。

### 5.2 触发 vLLM 预热（建议提前）

结合现有测试结论（低并发 1~3 fast 体验更好，但继续升高需尽快切 slow）：
- 建议在“并发达到 3 的尾段前”进入 `WARMING_VLLM`。
- 实现上可取：`C_prepare = min(3, C_up)`，当 `inflight_total >= C_prepare` 连续 2 个窗口即开始预热 `vLLM`。

### 5.3 扩容顺序

1. 突发到来时：
- 先扩 `llama.cpp`（启动快，先消化排队）。

2. 负载持续上升时：
- 并行启动/唤醒 `vLLM`。
- `vLLM` ready 后切入 `MIXED`，逐步提升慢后端流量占比。

3. 进入 `VLLM_PRIMARY` 后：
- 回收“缓冲型”`llama.cpp` 副本，仅保留 `L_floor`（建议 1）作为兜底。

### 5.4 缩容顺序（统一规则）

缩容目标：
- `L_target = max(L_floor, ceil(max(0, inflight_total - C_eff) / C_l))`，`C_l=1`（llama 串行假设）。

执行顺序：
1. 先缩 `llama.cpp` 缓冲实例（每周期最多 1 个，防抖）。
2. 低负载持续后再降 `vLLM`：`RUNNING -> SLEEP_1 -> SLEEP_2`。
3. 若继续低负载且无保活需求：`vLLM` 走 `DRAINING -> DELETING -> ABSENT`。

保护条件：
- 缩容前需 `inflight=0`（或先 `DRAINING`）。
- 任意时刻负载反弹，立即停止缩容并允许快速补 `llama.cpp`。

---

## 6. 请求路径与指标口径

请求路由原则：
- 单请求生命周期内不跨后端。
- 流式请求全程固定后端，避免体验抖动。

关键口径（与 benchmark 对齐）：
- `E2E = 请求发出到完整结果返回`（包含排队时间）。
- `TTFT = 请求发出到首 token 返回`。
- `queue_wait = E2E - (service_time)`，由客户端时间戳或网关时间戳推导。

延迟收益逻辑：
- 冷启动首波：fast 路径移除了 vLLM 启动等待，首字体验显著改善。
- 持续高压：切到 slow 路径后显著降低排队项，提升吞吐并压低 p95/p99。

---

## 7. 放置策略（软反亲和）

目标：
- 在资源允许时尽量把 `llama.cpp` 与 `vLLM` 放在不同 worker。
- 同一 alias 多副本也尽量分散。

评分建议：
- `Score = 0.45*S_mem + 0.30*S_anti_backend + 0.20*S_anti_replica - 0.05*S_load`

回退顺序：
1. 资源满足 + 双反亲和都满足。
2. 资源满足 + 满足其一。
3. 仅资源满足（可用性优先）。

---

## 8. 统一数据结构（Serve 内存态）

每个 alias 维护：
- `routing_state`：模型级状态（`COLD/FAST_ONLY/...`）。
- `fast_instances[]`：`llama.cpp` 实例清单（含 worker_id、state、inflight）。
- `slow_instances[]`：`vLLM` 实例清单（含 sleep_level/capacity）。
- `mix_weight`：`MIXED` 阶段分流权重。
- `switch_reason`：最近一次状态切换原因。
- `cooldown_until`：防抖冷却时间。
- `placement_state`：实例到 worker 的实时映射。

说明：
- Worker 不维护副本计数；副本分布由 Serve 基于实例清单实时计算。

---

## 9. 实施阶段（统一方案）

### 阶段 0：统一抽象与主控收敛

目标：
- 明确 Serve 是唯一决策主控。
- 落地统一实例状态枚举与 backend 能力矩阵。

完成依据：
- 代码中不存在 autoscaler 和 service 重复下发同类动作。
- `llama.cpp` 与 `vLLM` 都可被同一接口编排（含 unsupported 语义）。

### 阶段 1：接入 llama.cpp 为一等后端

目标：
- Worker/Adapter 层可启动、代理、drain、删除 `llama.cpp`。

完成依据：
- `FAST_ONLY` 路径稳定可用。
- 单实例 `llama.cpp` 删除后状态为 `ABSENT`（无挂名）。

### 阶段 2：模型级切换闭环

目标：
- 实现 `FAST_ONLY -> WARMING_VLLM -> MIXED -> VLLM_PRIMARY`。

完成依据：
- 达到阈值可触发预热，vLLM ready 后可平滑切换。
- vLLM 异常能回退 `DEGRADED_FAST` 且对外可用。

### 阶段 3：统一扩缩容

目标：
- 落地 `L_target`、drain 删除、vLLM 分级休眠。

完成依据：
- 负载回落后资源可自动回收且无请求丢失。
- 高负载反弹时可自动扩容并避免频繁震荡。

### 阶段 4：可观测与自动调参

目标：
- 增加结构化切换日志、队列指标与阈值自适应。

完成依据：
- 可查询每模型状态、阈值、目标副本、最近切换原因。
- 回放压测中切换次数下降或 p95 改善。

---

## 10. 验收标准

功能：
- 双后端在统一状态机下可切换、可回退、可扩缩。
- `llama.cpp` 与 `vLLM` 都遵循统一生命周期语义（含 `DRAINING/DELETING/ABSENT`）。

性能：
- 冷启动首波：TTFT/E2E 显著优于纯 vLLM。
- 持续高并发：吞吐与 `e2e_p95` 显著优于纯 llama.cpp。
- 切换阶段错误率不高于基线。

资源：
- 低负载时可自动释放冗余实例；高负载时可自动扩容。
- 不出现“资源已释放但状态仍可路由”的脏状态。

---

## 11. 默认参数建议（Qwen3-VL-2B）

- `HYBRID_ENABLE=1`
- `HYBRID_ALPHA=0.7`
- `HYBRID_BETA=0.3`
- `HYBRID_UP_CONSECUTIVE=2`
- `HYBRID_PREPARE_CONC=3`（并发到 3 前完成 vLLM 预热）
- `HYBRID_DOWN_HOLD_S=180`
- `HYBRID_MIX_WEIGHTS=20,50,80,100`
- `HYBRID_VLLM_WARM_TIMEOUT_S=180`
- `LLAMA_FLOOR_REPLICAS=1`
- `LLAMA_SCALE_DOWN_COOLDOWN_S=30`
- `LLAMA_DRAIN_TIMEOUT_S=120`
- `VLLM_SLEEP_L1_IDLE_S=300`
- `VLLM_SLEEP_L2_IDLE_S=900`
- `VLLM_DELETE_IDLE_S=1800`

---

## 12. 当前暂停时的代码改造计划（autoscaler 一体化主控）

说明：
- 按最新共识，`autoscaler` 是所有调度逻辑的唯一入口。
- `service` 只负责三件事：请求转发、实例动作执行、状态查询。
- 不做兼容层；采用破坏式收敛，优先可读性与维护性。

### 12.1 总体重构原则

1. 单一控制环
- 所有“切换/扩缩容/回退”决策仅在 `src/serve/autoscaler.py`。
- `src/serve/service.py` 不再包含独立调度策略。

2. 统一状态源
- 模型级状态：`routing_state`（`COLD/FAST_ONLY/WARMING_VLLM/MIXED/VLLM_PRIMARY/DEGRADED_FAST`）。
- 实例级状态：`ABSENT/STARTING/RUNNING/DRAINING/DELETING/ERROR`（vLLM 额外有 `SLEEP_1/SLEEP_2`）。
- 状态真值统一由 `autoscaler` 写入，`service` 只读并执行。

3. 无兼容约束
- 注册配置必须使用新结构（显式 `backends`）。
- 实例必须显式 `backend_type`，不再推断。
- 移除旧字段驱动的隐式逻辑（例如仅凭 `sleep_level` 判断所有后端）。

### 12.2 配置与数据结构收敛（先做）

目标文件：`src/serve/service.py`

新模型配置（唯一格式）：
```json
{
  "alias": "qwen3-vl-2b",
  "routing_state": "FAST_ONLY",
  "backends": {
    "llama.cpp": {
      "model_name": "...",
      "model_path": "...",
      "gpu_memory_gb": 6.0,
      "min_replicas": 1,
      "max_replicas": 4
    },
    "vllm": {
      "model_name": "...",
      "model_path": "...",
      "gpu_memory_gb": 12.0,
      "min_replicas": 0,
      "max_replicas": 2,
      "max_model_len": 4096,
      "tensor_parallel_size": 1
    }
  }
}
```

实例记录（统一字段）：
- `backend_type`（必填，`llama.cpp` 或 `vllm`）
- `active`、`pending_active`
- `status`、`inflight_requests`、`capacity`
- `sleep_level_value`（仅 vLLM 使用，llama.cpp 固定 0）

完成依据：
- 不传 `backends` 或缺失 `backend_type` 时直接报错。
- `list_models` 返回的实例可按 `backend_type` 明确分组。

### 12.3 autoscaler 融合快慢状态机（核心）

目标文件：`src/serve/autoscaler.py`

新增统一决策流程（每个 alias 每轮执行）：
1. `collect_metrics(alias)`
- 分 backend 聚合：`fast_inflight/slow_inflight/queue_wait_p95/e2e_p95/capacity_vllm`。

2. `decide_routing_state(alias)`
- 执行状态转移：
  - `FAST_ONLY -> WARMING_VLLM`
  - `WARMING_VLLM -> MIXED`
  - `MIXED -> VLLM_PRIMARY`
  - 异常 `-> DEGRADED_FAST`

3. `decide_target_replicas(alias)`
- 计算 `target_fast` 与 `target_slow`：
  - `target_slow` 由 `C_up/C_down` 与状态决定。
  - `target_fast` 用缓冲公式 `L_target` 计算。

4. `build_actions(alias)`
- 输出动作序列（有序）：`start -> wake -> route_weight_change -> drain -> delete`。

5. `apply_actions(alias)`
- 仅调用 `service` 提供的执行接口，不在 autoscaler 中写 HTTP 细节。

完成依据：
- `autoscaler` 日志可完整解释每次状态变化与动作原因。
- 关闭 `autoscaler` 后系统不再自动切换/扩缩（证明单主控成立）。

### 12.4 service 精简为执行与路由层

目标文件：`src/serve/service.py`

改造点：
- 保留实例操作原语：`start/wake/sleep/drain/delete/probe`。
- 路由只读 `routing_state + active实例列表`，不再自行推导切换策略。
- `resolve_instance_for_request` 按状态选 backend：
  - `FAST_ONLY/DEGRADED_FAST`：仅 fast
  - `WARMING_VLLM`：默认 fast
  - `MIXED`：按 `mix_weight` 采样 fast/slow
  - `VLLM_PRIMARY`：默认 slow，fast 兜底

完成依据：
- `service.py` 中不存在独立“扩缩容条件判断”。
- 所有调度阈值仅在 `autoscaler.py` 定义。

### 12.5 Worker 接口最小改造（为双后端执行服务）

目标文件：`src/worker/service.py`（以及后续 `llama.cpp` 管理实现）

约束：
- `/instances/start` 必须接收 `backend_type`。
- `backend_type=vllm` 走现有 `VLLMManager`。
- `backend_type=llama.cpp` 走 `LlamaManager`（待实现）。
- `/instances/{alias}/status` 返回统一字段，至少含：
  - `backend_type/status/inflight_requests/capacity`

完成依据：
- autoscaler 不需要分支猜测后端，直接读统一状态字段。

### 12.6 实施顺序（暂停后恢复时按此执行）

1. 先完成配置/数据结构硬收敛（12.2）。  
2. 再完成 autoscaler 决策主循环（12.3）。  
3. 然后删减 service 内策略逻辑（12.4）。  
4. 最后接入 llama.cpp Worker 执行器（12.5）。  

阶段验收（必须全过）：
- 冷启动首波：`FAST_ONLY` 可立即承接；
- 并发升高：自动进入 `WARMING_VLLM/MIXED/VLLM_PRIMARY`；
- 负载回落：先回收缓冲 fast，再降 slow（sleep/delete）；
- 全程无“双主控抢动作”日志。

---

## 13. 数学模型（简化版，面向参数标定）

目标：
- 面向当前系统关键矛盾建立最小模型：  
  - 快池（`llama.cpp`）：冷启动/唤醒快、吞吐较低；  
  - 慢池（`vllm`）：吞吐高、冷启动较慢；  
  - 慢池长时间不使用可进入休眠，休眠后显存占用显著降低。  
- 优化目标限定为三项：吞吐、端到端时延、显存占用。

### 13.1 建模对象与假设

对每个 `alias`，按离散时间片 `t=0,1,2,...`（周期为 `Delta`）建模。

状态变量（最小集合）：
- `q_t`：时间片开始时队列长度（待处理请求数）。
- `n_f_t`：快池活跃实例数。
- `n_s_t`：慢池活跃实例数。
- `z_s_t`：慢池休眠实例数。

输入与能力：
- `lambda_t`：请求到达率。
- `mu_f`：快池单实例服务率。
- `mu_s`：慢池单实例服务率（通常 `mu_s > mu_f`）。
- `tau_s_start`：慢池冷启动时延。
- `tau_s_wake`：慢池从休眠到可服务的唤醒时延。
- `m_f, m_s_act, m_s_slp`：快池活跃/慢池活跃/慢池休眠实例显存占用，且 `m_s_slp << m_s_act`。

### 13.2 控制变量

每个时间片由控制器（autoscaler）决定：
- `u_f_add_t`：快池新增数量。
- `u_f_del_t`：快池回收数量。
- `u_s_add_t`：慢池新增（启动）数量。
- `u_s_del_t`：慢池回收数量。
- `u_s_wake_t`：慢池唤醒数量。
- `u_s_slp_t`：慢池休眠数量。
- `w_t in [0,1]`：分流给快池的比例（慢池比例为 `1-w_t`）。

注：
- 若慢池休眠完全由执行面自动控制，则将 `u_s_wake_t/u_s_slp_t` 视作外生量；控制面仅优化其余动作。

### 13.3 动态方程（简化）

总服务能力：
- `C_f_t = mu_f * n_f_t`
- `C_s_t = mu_s * n_s_t`
- `C_t = C_f_t + C_s_t`

队列演化：
- `y_t = min(q_t + lambda_t * Delta, C_t * Delta)`  （本时间片完成请求数）
- `q_(t+1) = q_t + lambda_t * Delta - y_t`

快池活跃演化：
- `n_f_(t+1) = n_f_t + u_f_add_t - u_f_del_t`

慢池活跃/休眠演化（考虑启动与唤醒延迟）：
- `n_s_(t+1) = n_s_t + u_s_add_(t-tau_s_start) + u_s_wake_(t-tau_s_wake) - u_s_del_t - u_s_slp_t`
- `z_s_(t+1) = z_s_t + u_s_slp_t - u_s_wake_t`

### 13.4 优化目标函数

采用加权和目标（长期平均）：

`min Sum_t [ beta * D_t + gamma * M_t - alpha * y_t + eta * ||Delta u_t|| ]`

其中：
- `y_t`：吞吐（越大越好，因此前面是负号）。
- `D_t`：E2E 时延近似项，可用 `q_t / max(eps, C_t)` 近似排队等待。
- `M_t`：显存占用：
  - `M_t = m_f * n_f_t + m_s_act * n_s_t + m_s_slp * z_s_t`
- `||Delta u_t||`：动作变化惩罚，抑制频繁抖动。

`alpha, beta, gamma, eta` 是论文中可调权重，用于表达吞吐/时延/显存的取舍。

### 13.5 快池回收与慢池休眠协同策略（纳入模型）

为贴合系统行为，快池回收仍分两阶段：
- 软回收（路由层）：慢池就绪后降低 `w_t`，先减少快池接新流量。
- 硬回收（资源层）：当低负载持续时，再提升 `u_f_del_t`，减少快池活跃实例。

慢池侧显存优化由 `u_s_slp_t`（或执行面自动休眠）承担，从而在不牺牲慢池吞吐潜力的前提下降低显存占用。

### 13.6 与当前参数的映射（用于实验标定）

该简化模型可直接对应现有参数：
- 上行触发：`HYBRID_PREPARE_CONC`、`HYBRID_UP_CONSECUTIVE`。
- 下行触发：`SCALE_DOWN_LOAD_THRESHOLD`、`HYBRID_DOWN_HOLD_S`。
- 分流轨迹：`HYBRID_MIX_WEIGHTS`（对应 `w_t` 的离散路径）。
- 扩缩容节奏：`SCALE_UP_COOLDOWN`、`SCALE_DOWN_COOLDOWN`。
- 容量边界：`backends.{backend}.min_replicas/max_replicas`。

建议标定流程：
1. 从日志估计 `mu_f, mu_s, tau_s_start, tau_s_wake, m_f, m_s_act, m_s_slp`。  
2. 固定权重 `alpha,beta,gamma,eta` 后离线搜索参数组合。  
3. 用压测回放验证吞吐、E2E、显存三项指标并迭代。  

### 13.7 半解析求解流程（用于参数初值）

说明：
- 以下不是严格全局最优闭式解，而是“可计算的近似解析初值”。
- 目标是让调度参数从“拍脑袋”变成“由 metric 直接反推”。

步骤 1：构造单窗口近似成本函数
- 定义总能力：`C = mu_f * n_f + mu_s * n_s`。
- 定义慢池有效显存：`m_s_eff = (1 - p_slp) * m_s_act + p_slp * m_s_slp`。
- 定义近似目标：`J = beta / (C - lambda) + gamma * (m_f * n_f + m_s_eff * n_s)`。
- 其中 `lambda` 取窗口平均到达率，要求 `lambda < C`。

步骤 2：计算“扩一个实例是否值得”
- 对后端 `j in {f, s}`，定义增量代价近似：
- `DeltaJ_j ~= - beta * mu_j / ((C - lambda) * (C + mu_j - lambda)) + gamma * m_j + kappa * tau_j`。
- 快池取 `tau_f ~= 0`，慢池取 `tau_s ~= tau_s_start`（或唤醒场景用 `tau_s_wake`）。
- 若 `DeltaJ_j < 0`，则说明该后端在当前负载下应扩容。

步骤 3：反解上行负载阈值（扩容阈值）
- 令 `DeltaJ_j = 0` 可得到近似上行点：
- `lambda_up_j ~= C - sqrt( beta * mu_j / (gamma * m_j + kappa * tau_j) )`。
- 转换为负载阈值：
- `rho_up_j = lambda_up_j / C`。
- 工程设置可取 `SCALE_UP_LOAD_THRESHOLD ~= rho_up`（结合主后端取值）。

步骤 4：设置下行阈值（滞回）
- 为避免抖动，下行阈值设置为：
- `rho_down = rho_up - h`，其中 `h` 建议 `0.1 ~ 0.2`。
- 工程设置：`SCALE_DOWN_LOAD_THRESHOLD ~= rho_down`。

步骤 5：反解慢池预热触发点
- 预热原则：当“仅靠快池的排队等待”接近慢池启动时延时就应预热慢池。
- 判据：`Wq_fast(lambda) >= tau_s_start`。
- 单服务近似下可写：
- `Wq_fast(lambda) ~= lambda / (C_f * (C_f - lambda))`，`C_f = mu_f * n_f`。
- 反解可得：
- `lambda_prepare ~= tau_s_start * C_f^2 / (1 + tau_s_start * C_f)`。
- 可映射为并发触发值：`HYBRID_PREPARE_CONC ~= ceil(lambda_prepare / mu_f)`（初值）。

步骤 6：映射到当前参数
- `SCALE_UP_LOAD_THRESHOLD`：用步骤 3 的 `rho_up` 初值。
- `SCALE_DOWN_LOAD_THRESHOLD`：用步骤 4 的 `rho_down` 初值。
- `HYBRID_PREPARE_CONC`：用步骤 5 的 `lambda_prepare` 映射初值。
- `HYBRID_UP_CONSECUTIVE`：按噪声水平取 `1~3`（流量越抖取值越大）。
- `HYBRID_DOWN_HOLD_S`：可先取 `tau_s_start` 的 `2~4` 倍再实验修正。

步骤 7：上线前校准
- 用上述解析初值作为第一组参数。
- 在回放压测中微调，优先满足 `E2E/TTFT` 约束，再压显存。
